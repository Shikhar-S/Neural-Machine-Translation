{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NMT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkWMHEQVz_XI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "f292b80d-6169-4688-86ce-5385150029c2"
      },
      "source": [
        "!pip install indic-nlp-library"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting indic-nlp-library\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/58/8d1e621f87bbc4217fb8ce6628a2eb08b65a64582c5531becf41da5d721c/indic_nlp_library-0.6-py3-none-any.whl\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->indic-nlp-library) (1.12.0)\n",
            "Installing collected packages: morfessor, indic-nlp-library\n",
            "Successfully installed indic-nlp-library-0.6 morfessor-2.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QbhkstKz_eL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torch.utils.data import IterableDataset\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "import torch\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "import time\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrnv_N6nz_hB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ve_wuWoMz_jr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUA0yckKz_mv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_vocab_sz,input_embedding_dim,encoder_dim,decoder_dim):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embedding_layer=nn.Embedding(input_vocab_sz,input_embedding_dim)\n",
        "        self.rnn = nn.GRU(input_embedding_dim,encoder_dim,bidirectional=True)\n",
        "        self.forward_net = nn.Linear(encoder_dim * 2, decoder_dim)\n",
        "    \n",
        "    def forward(self,input,input_len):\n",
        "        #embed input\n",
        "        embeddings = self.embedding_layer(input)\n",
        "        packed_embeddings = nn.utils.rnn.pack_padded_sequence(embeddings,input_len,enforce_sorted=False)\n",
        "\n",
        "        #feed into rnn to get all hidden states\n",
        "        packed_hidden_states , last_hidden_state = self.rnn(packed_embeddings)\n",
        "        hidden_states, _ = nn.utils.rnn.pad_packed_sequence(packed_hidden_states) #unpack\n",
        "\n",
        "        #compute first hidden state for decoder\n",
        "        last_hidden_state = torch.tanh(self.forward_net(torch.cat((last_hidden_state[-2,:,:], last_hidden_state[-1,:,:]), dim = 1)))\n",
        "\n",
        "        return hidden_states, last_hidden_state\n",
        "\n",
        "# import torch\n",
        "# encoder = Encoder(12,18,24)\n",
        "# inp = torch.tensor([[2,3,4,5,7],[4,5,6,1,1],[6,5,1,1,1]]).long()\n",
        "# inpsz = torch.tensor([5,3,2])\n",
        "# hid = encoder(inp.permute(1,0),inpsz)\n",
        "# print(hid.shape)\n",
        "# print(hid)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioAOqf_Az_pW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,encoder_dim,decoder_dim):\n",
        "        super(Attention,self).__init__()\n",
        "        self.a = nn.Linear(encoder_dim*2 + decoder_dim,decoder_dim)\n",
        "        self.v = nn.Parameter(torch.rand(decoder_dim))\n",
        "        #attention computing model applies linear layer on concatenated encode, decoder inputs\n",
        "        #and then multiplies with a parameter to get dimension down to 1.\n",
        "\n",
        "    def forward(self,decoder_hidden,encoder_hiddens,mask):\n",
        "        #calculate e_ij\n",
        "        max_inp_sentence_length = encoder_hiddens.shape[0]\n",
        "        batch_size = encoder_hiddens.shape[1]\n",
        "\n",
        "        #step1 : linear layer\n",
        "        decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1,max_inp_sentence_length,1)\n",
        "        encoder_hiddens = encoder_hiddens.permute(1,0,2)\n",
        "        energy = torch.tanh(self.a(torch.cat((decoder_hidden,encoder_hiddens),dim=2)))\n",
        "\n",
        "        #step2 : mul with parameter to reduce dim\n",
        "        energy = energy.permute(0,2,1)\n",
        "        v = self.v.repeat(batch_size, 1).unsqueeze(1)\n",
        "        attention = torch.bmm(v, energy).squeeze(1)\n",
        "\n",
        "        #mask pad tokens\n",
        "        attention = attention.masked_fill(mask == 0, -1e10)\n",
        "        \n",
        "        return F.softmax(attention, dim = 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAWnwKgsz_st",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_vocab_sz,output_embedding_dim,encoder_dim,decoder_dim):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(output_vocab_sz,output_embedding_dim)\n",
        "        self.f = nn.GRU(output_embedding_dim + encoder_dim * 2  , decoder_dim)\n",
        "        self.g = nn.Linear(output_embedding_dim + decoder_dim + encoder_dim * 2,output_vocab_sz)\n",
        "        self.attention = Attention(encoder_dim,decoder_dim)\n",
        "\n",
        "    def forward(self,input,decoder_hidden,encoder_hiddens,mask):\n",
        "        #input = y_im1 , decoder_hidden = s_im1 , encoder_hiddens = h \n",
        "        \n",
        "        #compute attention\n",
        "        attn = self.attention(decoder_hidden,encoder_hiddens,mask)\n",
        "        \n",
        "        #compute weighted context vector c_i\n",
        "        attn = attn.unsqueeze(1)\n",
        "        encoder_hiddens = encoder_hiddens.permute(1, 0, 2)\n",
        "\n",
        "        c_i = torch.bmm(attn, encoder_hiddens)\n",
        "        \n",
        "        #compute new decoder hidden state\n",
        "        y_im1 = self.embedding_layer(input.unsqueeze(0))\n",
        "        c_i = c_i.permute(1,0,2)\n",
        "        rnn_input = torch.cat((y_im1,c_i), dim=2 ) \n",
        "\n",
        "        s_i, s_i_copy = self.f(rnn_input,decoder_hidden.unsqueeze(0)) \n",
        "\n",
        "        assert (s_i==s_i_copy).all()\n",
        "        \n",
        "        #compute next token\n",
        "        y_im1 = y_im1.squeeze(0)\n",
        "        s_i = s_i.squeeze(0)\n",
        "        c_i = c_i.squeeze(0)\n",
        "\n",
        "        y_i = self.g(torch.cat((s_i, c_i, y_im1), dim = 1))\n",
        "\n",
        "        return y_i, s_i_copy.squeeze(0), attn.squeeze(1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1XkvGYXYz_vF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,args,input_vocab_sz,output_vocab_sz,pad_idx, sos_idx, eos_idx):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.input_vocab_sz = input_vocab_sz\n",
        "        self.output_vocab_sz = output_vocab_sz\n",
        "        self.encoder = Encoder(input_vocab_sz,args['input_embedding_dim'],args['encoder_dim'],args['decoder_dim'])\n",
        "        self.decoder = Decoder(output_vocab_sz,args['output_embedding_dim'],args['encoder_dim'],args['decoder_dim])\n",
        "        self.pad_idx = pad_idx\n",
        "        self.sos_idx = sos_idx\n",
        "        self.eos_idx = eos_idx\n",
        "        self.device = args['device']\n",
        "        \n",
        "    def create_mask(self, src):\n",
        "        mask = (src != self.pad_idx).permute(1, 0)\n",
        "        return mask\n",
        "        \n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio = 0.5):\n",
        "     \n",
        "        #src = [src sent len, batch size]\n",
        "        #src_len = [batch size]\n",
        "        #trg = [trg sent len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
        "        \n",
        "        if trg is None:\n",
        "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
        "            inference = True\n",
        "            trg = torch.zeros((100, src.shape[1])).long().fill_(self.sos_idx).to(self.device)\n",
        "        else:\n",
        "            inference = False\n",
        "            \n",
        "        batch_size = src.shape[1]\n",
        "        max_len = trg.shape[0] if trg is not None else 100\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(max_len, batch_size, self.output_vocab_sz).to(self.device)\n",
        "        \n",
        "        #tensor to store attention\n",
        "        attentions = torch.zeros(max_len, batch_size, src.shape[0]).to(self.device)\n",
        "        \n",
        "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
        "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
        "        encoder_hiddens, hidden_last = self.encoder(src, src_len)\n",
        "                \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        output = trg[0,:]\n",
        "        \n",
        "        mask = self.create_mask(src)\n",
        "                \n",
        "        #mask = [batch size, src sent len]\n",
        "                \n",
        "        for t in range(1, max_len):\n",
        "            output, hidden_last, attention = self.decoder(output, hidden_last, encoder_hiddens, mask)\n",
        "            outputs[t] = output\n",
        "            attentions[t] = attention\n",
        "            \n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.max(1)[1]\n",
        "            output = (trg[t] if (teacher_force and not inference) else top1)\n",
        "            if inference and output.item() == self.eos_idx:\n",
        "                return outputs[:t], attentions[:t]\n",
        "            \n",
        "        return outputs, attentions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKsX4C8z_x6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def en_preprocessor(text):\n",
        "    return [t.lower().replace('.','') for t in text.split()]\n",
        "\n",
        "def hi_preprocessor(text):\n",
        "    return [token for token in indic_tokenize.trivial_tokenize(text)]\n",
        "\n",
        "def collator(batch,PAD_IDX):\n",
        "    max_src_len = max_trg_len = 0\n",
        "    for x,y in batch:\n",
        "        max_src_len = max(max_src_len,len(x))\n",
        "        max_trg_len = max(max_trg_len,len(y))\n",
        "    X=[]\n",
        "    X_len= []\n",
        "    Y=[]\n",
        "    for x,y in batch:\n",
        "        X.append(x+[PAD_IDX for i in range(max_src_len-len(x))])\n",
        "        X_len.append(len(x))\n",
        "        Y.append(y+[PAD_IDX for i in range(max_trg_len-len(y))])\n",
        "    \n",
        "    Y=torch.tensor(Y).permute(1,0).contiguous()\n",
        "    X=torch.tensor(X).permute(1,0).contiguous()\n",
        "    X_len =torch.tensor(X_len)\n",
        "    return (X,X_len),Y\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self,src_dic=None,trg_dic=None):\n",
        "        self.src_stoi = src_dic\n",
        "        self.src_itos = defaultdict(self.ret_unk)\n",
        "       \n",
        "        if self.src_stoi is not None:\n",
        "            for k,v in self.src_stoi.items():\n",
        "                self.src_itos[v]=k\n",
        "\n",
        "        self.trg_stoi = trg_dic\n",
        "        self.trg_itos = defaultdict(self.ret_unk)\n",
        "        \n",
        "        if self.trg_stoi is not None:\n",
        "            for k,v in self.trg_stoi.items():\n",
        "                self.trg_itos[v]=k\n",
        "    \n",
        "    def ret_z(self):\n",
        "        return 0\n",
        "    def ret_unk(self):\n",
        "        return '<UNK>'\n",
        "    \n",
        "    def build_dic(self,path,preprocessor):\n",
        "        dic=defaultdict(self.ret_z)\n",
        "        dic['<sos>']=1\n",
        "        dic['<eos>']=2\n",
        "        dic['<pad>']=3\n",
        "        ctr =  4\n",
        "        with open(path,'r') as F:\n",
        "            for line in F:\n",
        "                for token in preprocessor(line):\n",
        "                    if token not in dic:\n",
        "                        dic[token]=ctr\n",
        "                        ctr+=1\n",
        "        return dic\n",
        "    \n",
        "    def add_src_dic(self,dic):\n",
        "        self.src_stoi = dic\n",
        "        for k,v in self.src_stoi.items():\n",
        "            self.src_itos[v]=k\n",
        "    \n",
        "    def add_trg_dic(self,dic):\n",
        "        self.trg_stoi = dic\n",
        "        for k,v in self.trg_stoi.items():\n",
        "            self.trg_itos[v]=k\n",
        "\n",
        "class DataReader(IterableDataset):\n",
        "    def __init__(self,args,paths,src_preprocessor,trg_preprocessor,DIC=None):\n",
        "        self.src_path = paths[0]\n",
        "        self.trg_path = paths[1]\n",
        "        \n",
        "        self.vocab = Vocab()\n",
        "        if DIC is None:\n",
        "            src_dic = self.vocab.build_dic(self.src_path,src_preprocessor)\n",
        "            trg_dic = self.vocab.build_dic(self.trg_path,trg_preprocessor)\n",
        "            self.vocab.add_src_dic(src_dic)\n",
        "            self.vocab.add_trg_dic(trg_dic)\n",
        "        else:\n",
        "            self.vocab=DIC\n",
        "        \n",
        "        self.src_preprocessor = src_preprocessor\n",
        "        self.trg_preprocessor = trg_preprocessor\n",
        "\n",
        "    def line_mapper(self, line, is_src):\n",
        "        text = line\n",
        "        tokens = []\n",
        "        if is_src:\n",
        "            tokens.append(self.vocab.src_stoi['<sos>'])\n",
        "            tokens = tokens + [self.vocab.src_stoi[token] for token in self.src_preprocessor(text)]\n",
        "            tokens.append(self.vocab.src_stoi['<eos>'])\n",
        "        else:\n",
        "            tokens.append(self.vocab.trg_stoi['<sos>'])\n",
        "            tokens = tokens + [self.vocab.trg_stoi[token] for token in self.trg_preprocessor(text)]\n",
        "            tokens.append(self.vocab.trg_stoi['<eos>'])\n",
        "        return tokens\n",
        "\n",
        "    def __iter__(self):\n",
        "        #Create an iterator\n",
        "        src_itr = open(self.src_path)\n",
        "        trg_itr = open(self.trg_path)\n",
        "        \n",
        "        #Map each element using the line_mapper\n",
        "        mapped_src_itr = map(lambda text : self.line_mapper(text,True), src_itr)\n",
        "        mapped_trg_itr = map(lambda text : self.line_mapper(text,False), trg_itr)\n",
        "        \n",
        "        #Zip both iterators\n",
        "        zipped_itr = zip(mapped_src_itr, mapped_trg_itr)\n",
        "        \n",
        "        return zipped_itr\n",
        "\n",
        "# #TEST\n",
        "# import config\n",
        "# args,unparsed = config.get_args()\n",
        "# test_dataset = DataReader(args,('./Data/dev_test/dev.en','./Data/dev_test/dev.hi'),en_preprocessor,hi_preprocessor)\n",
        "# print('built vocab')\n",
        "# dataloader = DataLoader(test_dataset, batch_size = 4, drop_last=True,collate_fn= lambda b: collator(b,3))\n",
        "\n",
        "# for X, y in dataloader:\n",
        "#     print(X)\n",
        "#     print()\n",
        "#     print(y)\n",
        "#     break\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0uuwMqJz7H5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip, args):\n",
        "    device=args['device']\n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    batch_ctr=0\n",
        "    for batch in tqdm(iterator):\n",
        "        \n",
        "        src, src_len = batch[0]\n",
        "        trg = batch[1]\n",
        "        src=src.to(device)\n",
        "        src_len=src_len.to(device)\n",
        "        trg = trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, src_len, trg)\n",
        "        \n",
        "        #trg = [trg sent len, batch size]\n",
        "        #output = [trg sent len, batch size, output dim]\n",
        "        \n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        \n",
        "        #trg = [(trg sent len - 1) * batch size]\n",
        "        #output = [(trg sent len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        batch_ctr+=1\n",
        "    return epoch_loss / (batch_ctr*args['batch'])\n",
        "\n",
        "def evaluate(model, iterator, criterion, args):\n",
        "    device=utils.get_device(args)\n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    batch_ctr=0\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in tqdm(iterator):\n",
        "\n",
        "            src, src_len = batch[0]\n",
        "            trg = batch[1]\n",
        "\n",
        "            src=src.to(device)\n",
        "            src_len=src_len.to(device)\n",
        "            trg = trg.to(device)\n",
        "\n",
        "            output, _ = model(src, src_len, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg sent len, batch size]\n",
        "            #output = [trg sent len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg sent len - 1) * batch size]\n",
        "            #output = [(trg sent len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            batch_ctr+=1\n",
        "        \n",
        "    return epoch_loss / (batch_ctr*args['batch'])\n",
        "\n",
        "def translate_sentence(model,vocab,sentence,args):\n",
        "    model.eval()\n",
        "    device = args['device']\n",
        "    tokenized = en_preprocessor(sentence) \n",
        "    tokenized = ['<sos>'] + tokenized + ['<eos>']\n",
        "    numericalized = [vocab.src_stoi[t] for t in tokenized] \n",
        "    sentence_length = torch.LongTensor([len(numericalized)]).to(device) \n",
        "    tensor = torch.LongTensor(numericalized).unsqueeze(1).to(device) \n",
        "    translation_tensor_logits, attention = model(tensor, sentence_length, None) \n",
        "    translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1)\n",
        "    translation = [vocab.trg_itos[t] for t in translation_tensor]\n",
        "    translation, attention = translation[1:], attention[1:]\n",
        "    return translation, attention\n",
        "\n",
        "def display_attention(candidate, translation, attention):\n",
        "    fig = plt.figure(figsize=(10,10))\n",
        "    ax = fig.add_subplot(111)\n",
        "    \n",
        "    attention = attention.squeeze(1).cpu().detach().numpy()\n",
        "    \n",
        "    cax = ax.matshow(attention, cmap='bone')\n",
        "   \n",
        "    ax.tick_params(labelsize=15)\n",
        "    ax.set_xticklabels([''] + ['<sos>'] + [t.lower() for t in en_preprocessor(candidate)] + ['<eos>'], \n",
        "                       rotation=45)\n",
        "    ax.set_yticklabels([''] + translation)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def translation_mode(args):\n",
        "    vocab=None\n",
        "    with open(args.load_dic_path, 'rb') as F:\n",
        "        vocab = pickle.load(F)\n",
        "\n",
        "    INPUT_DIM = len(vocab.src_stoi)\n",
        "    OUTPUT_DIM = len(vocab.trg_stoi)\n",
        "    PAD_IDX = vocab.src_stoi['<pad>']\n",
        "    SOS_IDX = vocab.src_stoi['<sos>']\n",
        "    EOS_IDX = vocab.src_stoi['<eos>']\n",
        "    device = utils.get_device(args)\n",
        "\n",
        "    model = Seq2Seq(args,INPUT_DIM,OUTPUT_DIM, PAD_IDX, SOS_IDX, EOS_IDX).to(device)\n",
        "    model.load_state_dict(torch.load(args.load_model_path))\n",
        "\n",
        "    sentence=input('Enter sentence in source language')\n",
        "    translation,attention = translate_sentence(model,vocab,sentence,args)\n",
        "    print('Translated: ',' '.join(translation.join))\n",
        "    display_attention(sentence,translation,attention)    \n",
        "\n",
        "def train_mode(args):\n",
        "    #Get Data\n",
        "    training_dataset = DataReader(args,args['training_data'],en_preprocessor,hi_preprocessor)\n",
        "    validation_dataset = DataReader(args,args['validation_data'],en_preprocessor,hi_preprocessor,training_dataset.vocab)\n",
        "    # testing_dataset = DataReader(args,args.testing_data,en_preprocessor,hi_preprocessor,training_dataset.vocab)\n",
        "    \n",
        "    INPUT_DIM = len(training_dataset.vocab.src_stoi)\n",
        "    OUTPUT_DIM = len(training_dataset.vocab.trg_stoi)\n",
        "\n",
        "    device = utils.get_device(args)\n",
        "\n",
        "    PAD_IDX = training_dataset.vocab.src_stoi['<pad>']\n",
        "    SOS_IDX = training_dataset.vocab.src_stoi['<sos>']\n",
        "    EOS_IDX = training_dataset.vocab.src_stoi['<eos>']\n",
        "\n",
        "    training_dataloader = DataLoader(training_dataset, batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX))\n",
        "    validation_dataloader = DataLoader(validation_dataset,batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX))\n",
        "    # testing_dataloader = DataLoader(testing_dataset,batch_size = args.batch, drop_last=True, collate_fn=lambda b: collator(b,PAD_IDX))\n",
        "\n",
        "    #Get model\n",
        "    model = Seq2Seq(args,INPUT_DIM,OUTPUT_DIM, PAD_IDX, SOS_IDX, EOS_IDX).to(device)\n",
        "    logger.info(model.apply(utils.init_weights),extra=args.exec_id) #init model\n",
        "    logger.info(\"Number of trainable parameters: \"+str(utils.count_parameters(model)),extra=args.exec_id) #log Param count\n",
        "\n",
        "    #Train and Evaluate model\n",
        "    N_EPOCHS = args.epochs\n",
        "    CLIP = 1\n",
        "    best_valid_loss = float('inf')\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters())\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)\n",
        "    \n",
        "    for epoch in range(N_EPOCHS): \n",
        "        start_time = time.time()\n",
        "        \n",
        "        train_loss = train(model, training_dataloader, optimizer, criterion, CLIP, args)\n",
        "        valid_loss = evaluate(model, validation_dataloader, criterion, args)\n",
        "        \n",
        "        end_time = time.time()\n",
        "        \n",
        "        epoch_mins, epoch_secs = utils.epoch_time(start_time, end_time)\n",
        "        \n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), args['save_model_path'])\n",
        "            with open(args['save_dic_path'],'wb') as F:\n",
        "                pickle.dump(training_dataset.vocab.src_stoi,F)\n",
        "        \n",
        "        print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLyDK9332XRF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "970a7468-4b3b-40e2-951c-3e9641ef3ae0"
      },
      "source": [
        "args={\n",
        "    'batch':16,\n",
        "    'input_embedding_dim':128,\n",
        "    'output_embedding_dim':128,\n",
        "    'encoder_dim':512,\n",
        "    'decoder_dim':512,\n",
        "    'epochs':10,\n",
        "    'device':'cpu',\n",
        "    'training_data':('./Data/parallel/IITB.en-hi.en','./Data/parallel/IITB.en-hi.hi'),\n",
        "    'testing_data':('./Data/dev_test/test.en','./Data/dev_test/test.hi'),\n",
        "    'validation_data':('./Data/dev_test/dev.en','./Data/dev_test/dev.hi'),\n",
        "    'save_model_path':'./trained_models/seq2seq.pt',\n",
        "    'save_dic_path':'./trained_models/dictionary.pkl',\n",
        "    'load_model_path':'./trained_models/seq2seq.pt',\n",
        "    'load_dic_path':'./trained_models/dictionary.pkl'\n",
        "}\n",
        "train_mode(args)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-d57ee91b4666>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m'load_dic_path'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'./trained_models/dictionary.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m }\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-c0668e41b967>\u001b[0m in \u001b[0;36mtrain_mode\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m#Get Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mtraining_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhi_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m     \u001b[0mvalidation_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation_data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0men_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhi_preprocessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# testing_dataset = DataReader(args,args.testing_data,en_preprocessor,hi_preprocessor,training_dataset.vocab)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8c156a4e6513>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, paths, src_preprocessor, trg_preprocessor, DIC)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDIC\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0msrc_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msrc_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msrc_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0mtrg_dic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrg_preprocessor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_src_dic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc_dic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-8c156a4e6513>\u001b[0m in \u001b[0;36mbuild_dic\u001b[0;34m(self, path, preprocessor)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mdic\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mctr\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './Data/parallel/IITB.en-hi.en'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtpzcH0s3tFD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}